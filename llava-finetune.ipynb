{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10737f6-324e-4f44-8db8-922ee416ee58",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tuning the multimodal LLaVa model to identify potential male models\n",
    "\n",
    "In this notebook we'll fine-tune the LLaVA model from [Haotian Liu](https://github.com/haotian-liu/LLaVA) to identify young men who fall under the categories of \"pretty boy\" and \"good looking.\" The purpose of this fine-tuning is to create a version of the LLaVa model that can be used by male modeling agencies to find men who are not necessarily models but have the potential to be so. When images are submitted to this fine-tuned model, the model will tell the user whether the men in the image are \"pretty,\" as well as other aspects, such as their race. In doing so, we hope to diversify the current modeling market and facilitate the recruiting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5808bde-44da-4588-95f4-68c292cf9400",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table of contents \n",
    "\n",
    "1. Data Preprocessing\n",
    "2. LLaVA Installation\n",
    "3. DeepSpeed configuration\n",
    "4. Weights and Biases\n",
    "5. Finetuning flow\n",
    "6. Deployment via gradio interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2638c4-fea4-451d-a512-4e2c731cbdec",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "LLaVA requires data to be in a very specific format. Below we use a helper function to format the required dataset from hugging face: [jcthehaxer/check](https://huggingface.co/datasets/jcthehaxer/check). This dataset teaches the model what an attractive young man looks like and what to look for. To process the dataset effectively, first download the dataset from hugging face. Then, create a new folder in your workspace and call it \"datasets.\" In the new folder, create yet another folder, this time calling it \"images\". Upload all of the image and the metadata.csv files to \"images\". Next, run the cell that contains the helper function. The helper function will split the data into training and testing subsets, which will be outputed as two JSON files: \"train_llava_data.json\" and \"val_llava_data.json\". Once these files have been returned, create two new folders in \"datasets\" and name them \"train\" and \"validation\". Move each JSON file to its respective folder. Finally, once each JSON file is in its own folder, rename both files to \"datasets.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf7a70-0294-4a75-bffe-d0c375ce11ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install preprocessing libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade --force-reinstall Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe5a05-a956-483d-ad5f-df940a140bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "def create_llava_finetune_data(image_folder, metadata_file, train_output_file, val_output_file, split_ratio=0.8):\n",
    "    # Dictionary to hold image-to-answer mapping from metadata.csv\n",
    "    image_answer_map = {}\n",
    "\n",
    "    # Read the metadata.csv file to extract the file_name and corresponding answers\n",
    "    with open(metadata_file, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        next(csv_reader)  # Skip the header row if present\n",
    "        for row in csv_reader:\n",
    "            file_name, answer = row[0], row[1]\n",
    "            image_answer_map[file_name] = answer\n",
    "\n",
    "    # List to hold all the formatted data\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate through the image files in the folder\n",
    "    for filename in os.listdir(image_folder):\n",
    "        # Only process image files\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')):\n",
    "            # Generate a unique UUID\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Check if the image has a corresponding answer in the metadata\n",
    "            if filename in image_answer_map:\n",
    "                # Retrieve the answer from the CSV data\n",
    "                answer = image_answer_map[filename]\n",
    "                \n",
    "                # Construct the new image file name based on the UUID\n",
    "                new_image_name = f\"{unique_id}.jpg\"\n",
    "                new_image_path = os.path.join(image_folder, new_image_name)\n",
    "                \n",
    "                # Rename the image file to match the UUID\n",
    "                old_image_path = os.path.join(image_folder, filename)\n",
    "                shutil.move(old_image_path, new_image_path)  # Renames the file\n",
    "                \n",
    "                # Construct the formatted JSON data for LLaVA\n",
    "                json_data = {\n",
    "                    \"id\": unique_id,  # UUID as the unique ID\n",
    "                    \"image\": new_image_name,  # Use the UUID for the image name\n",
    "                    \"conversations\": [\n",
    "                        {\n",
    "                            \"from\": \"human\",\n",
    "                            \"value\": \"What do you see?\"  # Fixed question\n",
    "                        },\n",
    "                        {\n",
    "                            \"from\": \"gpt\",\n",
    "                            \"value\": answer  # Answer from the metadata.csv\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                # Append the formatted data to the list\n",
    "                all_data.append(json_data)\n",
    "\n",
    "    # Shuffle the data to ensure randomness\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    # Split the data into train and validation/test sets based on the split ratio\n",
    "    train_size = int(len(all_data) * split_ratio)\n",
    "    train_data = all_data[:train_size]\n",
    "    val_data = all_data[train_size:]\n",
    "\n",
    "    # Write the training data to a JSON file\n",
    "    with open(train_output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(train_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Write the validation/test data to a separate JSON file\n",
    "    with open(val_output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(val_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Training data saved to {train_output_file}\")\n",
    "    print(f\"Validation/Test data saved to {val_output_file}\")\n",
    "    print(f\"Images have been renamed using UUIDs.\")\n",
    "\n",
    "# Example usage\n",
    "image_folder = 'dataset/images'  # Folder containing images\n",
    "metadata_file = 'dataset/images/metadata.csv'  # Path to metadata.csv\n",
    "train_output_file = 'train_llava_data.json'  # Output JSON file for training data\n",
    "val_output_file = 'val_llava_data.json'  # Output JSON file for validation/test data\n",
    "\n",
    "create_llava_finetune_data(image_folder, metadata_file, train_output_file, val_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078bcba-0a3f-4818-a745-2f3cbdff7c3a",
   "metadata": {},
   "source": [
    "## Install LLaVA\n",
    "\n",
    "To install the functions needed to use the model, we have to clone the original LLaVA repository and and install it in editable mode. This lets us access all functions and helper methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0387008-d4ad-4c58-bb9d-d77f71637257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The pip install -e . lets us install the repository in editable mode\n",
    "!git clone https://github.com/haotian-liu/LLaVA.git\n",
    "!cd LLaVA && pip install --upgrade pip && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa03b91-85a8-434a-81f6-105dfa87ecb2",
   "metadata": {},
   "source": [
    "## DeepSpeed\n",
    "\n",
    "Microsoft DeepSpeed is a deep learning optimization library designed to enhance the training speed and scalability of large-scale artificial intelligence (AI) models. Developed by Microsoft, this open-source tool specifically addresses the challenges associated with training very large models, allowing for reduced computational times and resource usage. By optimizing memory management and introducing novel parallelism techniques, DeepSpeed enables developers and researchers to train models with billions of parameters efficiently, even on limited hardware setups. DeepSpeed API is a lightweight wrapper on PyTorch. DeepSpeed manages all of the boilerplate training techniques, such as distributed training, mixed precision, gradient accumulation, and checkpoints and allows you to just focus on model development. To learn more about DeepSpeed and how it performs the magic, check out this [article](https://www.deepspeed.ai/2021/03/07/zero3-offload.html) on DeepSpeed and ZeRO.\n",
    "\n",
    "Using deepspeed is extremely simple - you simply pip install it! The LLaVA respository contains the setup scripts and configuration files needed to finetune in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55721ad3-f88f-4e03-9d99-d193c276bd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd LLaVA && pip install -e \".[train]\"\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d16e09-a3ec-461b-8a95-78c3a4e22379",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ac37a-10ad-46b9-84f7-ef289a29bbbb",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "Weights and Biases is an industry standard MLOps tool used to monitor and evaluate training jobs. Make sure to have a Weights and Biases account before continuing. After fine-tuning, you will be able to see a report of the run on your Weights and Biases workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae96a8-f281-471c-aeb9-dac6bc7f5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f3d47-8daa-480a-acd7-6517bac50c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc74b4e-6c88-4e9e-92bd-610a54fa01a3",
   "metadata": {},
   "source": [
    "## Finetuning job\n",
    "\n",
    "Below we start the DeepSpeed training run for 5 epochs. It will automatically recognize multiple GPUs and parallelize across them. Most of the input flags are standard but you can adjust your training run with the `num_train_epochs` and `per_device_train_batch_size` flags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2019238-2ce5-4985-98c1-7dabf9c10169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed LLaVA/llava/train/train_mem.py \\\n",
    "    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed LLaVA/scripts/zero3.json \\\n",
    "    --model_name_or_path liuhaotian/llava-v1.5-13b \\\n",
    "    --version v1 \\\n",
    "    --data_path ./dataset/train/dataset.json \\\n",
    "    --image_folder ./dataset/images \\\n",
    "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dca5d-8014-4c28-a922-c24187d9db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the LoRA weights with the full model\n",
    "!python LLaVA/scripts/merge_lora_weights.py --model-path checkpoints/llava-v1.5-13b-task-lora --model-base liuhaotian/llava-v1.5-13b --save-model-path llava-ftmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52c548-a1b4-477a-ad68-1d9ac153fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bump transformers down for gradio/deployment inference if needed\n",
    "!pip install transformers==4.37.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f3d67-3c9c-4413-b86d-0a4ec2883df8",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "LLaVA gives us 2 ways to deploy the model - via CLI or Gradio UI. You can test out the fine-tuned model with the cells below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34046faf-3e2c-4726-9e41-6f4bb3028dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the lines below to run the CLI. You need to pass in a JPG image URL to use the multimodal capabilities\n",
    "\n",
    "!python -m llava.serve.cli \\\n",
    "     --model-path llava-ftmodel \\\n",
    "     --image-file \"image url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a9a5a-d386-4b9b-a9f6-808217d6f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model runner\n",
    "!wget -L https://raw.githubusercontent.com/brevdev/notebooks/main/assets/llava-deploy.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9550ef4-e881-49d7-a237-a997234d179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference! Use the public link provided in the output to test\n",
    "!chmod +x llava-deploy.sh && ./llava-deploy.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
